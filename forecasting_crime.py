# -*- coding: utf-8 -*-
"""forecasting_crime_for_the_city_of_tempe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CgLZzoExKOs-Qu86iMFB0hLZM6cGccai
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
file_path = '/content/drive/Shareddrives/dm_final/dm_final/data/gridcentroid_dataframe_07x07_weekday.csv.xls'
data = pd.read_csv(file_path)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Selecting relevant features and the target variable
features = data[['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart',
                 'IsWeekend', 'GridID', 'X_centroid', 'Y_centroid']]
target = data['NumberOfIncidents']

# Simplified parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 150],
    'learning_rate': [0.1, 0.15],
    'max_depth': [3, 4]
}

# Setting up GridSearchCV
grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42),
                           param_grid=param_grid,
                           scoring='r2',
                           cv=3,
                           verbose=1,
                           n_jobs=-1)

# Fitting GridSearchCV to the data
grid_search.fit(features,target)

# Best parameters and score from the grid search
best_params = grid_search.best_params_
best_score = grid_search.best_score_

# Training the model with the best parameters
best_gb_model = GradientBoostingRegressor(**best_params, random_state=42)
best_gb_model.fit(features, target)

print("Best Parameters:", best_params)
print("Best Grid Search Score:", best_score)

import pandas as pd
file_path = '/content/drive/Shareddrives/dm_final/dm_final/data/test_gridcentroid_test_07x07_weekday.csv.xls'
test_data = pd.read_csv(file_path)

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Re-train the Gradient Boosting Regressor with the best parameters
# Preparing the test data for prediction
X_test = test_data[['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart',
                    'IsWeekend', 'GridID', 'X_centroid', 'Y_centroid']]
y_test = test_data['NumberOfIncidents']

# Making predictions on the test data
y_pred_test = best_gb_model.predict(X_test)

# Calculating performance metrics
mse_test = mean_squared_error(y_test, y_pred_test)
rmse_test = np.sqrt(mse_test)
mae_test = mean_absolute_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

mse_test, rmse_test, mae_test, r2_test

y_pred_test

import pandas as pd

y_pred_series = pd.Series(y_pred_test, name='PredictedNumberOfIncidents')

# Merge the predictions with the X_test dataframe
merged_df = pd.concat([X_test, y_pred_series], axis=1)

# Now merged_df contains both the features and the predicted number of incidents
print(merged_df.head())

updated_dataframe_path = '/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted.csv'
merged_df.to_csv(updated_dataframe_path, index=False)

import warnings
warnings.filterwarnings('ignore')
# Extract unique dates and grid IDs
unique_dates_final_predicted = final_predicted_df['OccurrenceDatePart'].unique()
unique_grid_ids = grid_mapping_df['GridID'].unique()

# Initialize an empty DataFrame to store the updated data
updated_final_predicted_df = pd.DataFrame()

for date in unique_dates_final_predicted:
    for grid_id in unique_grid_ids:
        existing_record = final_predicted_df[(final_predicted_df['OccurrenceDatePart'] == date) & (final_predicted_df['GridID'] == grid_id)]

        if existing_record.empty:
            grid_info = grid_mapping_df[grid_mapping_df['GridID'] == grid_id]
            new_record = {
                'OccurrenceYear': existing_record.iloc[0]['OccurrenceYear'] if not existing_record.empty else 2023,
                'OccurrenceMonth': existing_record.iloc[0]['OccurrenceMonth'] if not existing_record.empty else 10,
                'OccurrenceDatePart': date,
                'IsWeekend': existing_record.iloc[0]['IsWeekend'] if not existing_record.empty else 0,
                'GridID': grid_id,
                'X_centroid': grid_info.iloc[0]['X_centroid'] if 'X_centroid' in grid_info.columns else None,
                'Y_centroid': grid_info.iloc[0]['Y_centroid'] if 'Y_centroid' in grid_info.columns else None,
                'PredictedNumberOfIncidents': 0,
                'geometry': grid_info.iloc[0]['geometry']
            }
            updated_final_predicted_df = updated_final_predicted_df.append(new_record, ignore_index=True)
        else:
            updated_final_predicted_df = updated_final_predicted_df.append(existing_record)

# Resetting index of the updated DataFrame
updated_final_predicted_df.reset_index(drop=True, inplace=True)
updated_final_predicted_df

updated_final_predicted_df.drop(columns=['geometry'])
updated_final_predicted_df.head(100)

import pandas as pd
from itertools import product

final_predicted_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted.csv')
grid_mapping_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/grid_mapping_7x7.csv')

# Extract unique values for year, month, and date part
unique_years = final_predicted_df['OccurrenceYear'].unique()
unique_months = final_predicted_df['OccurrenceMonth'].unique()
unique_date_parts = final_predicted_df['OccurrenceDatePart'].unique()

# Extract grid IDs from the grid mapping file
unique_grid_ids = grid_mapping_df['GridID'].unique()

# Generate all possible combinations
all_combinations = list(product(unique_years, unique_months, unique_date_parts, unique_grid_ids))

# Convert these combinations into a DataFrame
all_combinations_df = pd.DataFrame(all_combinations, columns=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'])

# Identify missing combinations in final_predicted.csv
missing_combinations_df = all_combinations_df[~all_combinations_df.isin(final_predicted_df).all(axis=1)]

# Create a new DataFrame with missing combinations and incidents set to 0
missing_combinations_df['PredictedNumberOfIncidents'] = 0

# Merge the original data with the missing combinations
complete_data_df = pd.concat([final_predicted_df, missing_combinations_df], ignore_index=True)

# Sort the data for better organization
complete_data_df.sort_values(by=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'], inplace=True)

# Reset the index
complete_data_df.reset_index(drop=True, inplace=True)

# Now complete_data_df contains the full dataset with missing entries filled
complete_data_df

import pandas as pd
from itertools import product

final_predicted_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted.csv')
grid_mapping_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/grid_mapping_7x7.csv')

# Extract unique values for year, month, and date part
unique_years = final_predicted_df['OccurrenceYear'].unique()
unique_months = final_predicted_df['OccurrenceMonth'].unique()
unique_date_parts = final_predicted_df['OccurrenceDatePart'].unique()

# Extract grid IDs from the grid mapping file
unique_grid_ids = grid_mapping_df['GridID'].unique()

# Generate all possible combinations
all_combinations = list(product(unique_years, unique_months, unique_date_parts, unique_grid_ids))

# Convert these combinations into a DataFrame
all_combinations_df = pd.DataFrame(all_combinations, columns=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'])

# Merge the original data with all possible combinations using a left join
complete_data_df = pd.merge(all_combinations_df, final_predicted_df,
                            on=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'],
                            how='left')

# Fill NaN values in PredictedNumberOfIncidents with 0
complete_data_df['PredictedNumberOfIncidents'].fillna(0, inplace=True)

# Sort the data for better organization
complete_data_df.sort_values(by=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'], inplace=True)

# Reset the index
complete_data_df.reset_index(drop=True, inplace=True)

# Now complete_data_df contains the full dataset with original values

complete_data_df

import pandas as pd
from itertools import product

final_predicted_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted.csv')
grid_mapping_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/grid_mapping_7x7.csv')

# Extract unique values for year, month, and date part from final_predicted.csv
unique_years_months_dates = final_predicted_df[['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart']].drop_duplicates()

# Extract grid IDs from the grid mapping file
unique_grid_ids = grid_mapping_df['GridID'].unique()

# Generate all possible combinations of year, month, date part, and grid ID
all_combinations = product(unique_years_months_dates.itertuples(index=False), unique_grid_ids)
all_combinations_df = pd.DataFrame([(*ymd, grid_id) for ymd, grid_id in all_combinations],
                                   columns=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'])

# Merge the original data with all possible combinations using a left join
complete_data_df = pd.merge(all_combinations_df, final_predicted_df,
                            on=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'],
                            how='left')

# Fill NaN values in PredictedNumberOfIncidents with 0
complete_data_df['PredictedNumberOfIncidents'].fillna(0, inplace=True)

# Sort the data for better organization
complete_data_df.sort_values(by=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'], inplace=True)

# Reset the index
complete_data_df.reset_index(drop=True, inplace=True)

import pandas as pd
from itertools import product

final_predicted_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted.csv')
grid_mapping_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/grid_mapping_7x7.csv')

# Extract unique values for year, month, and date part from final_predicted.csv
unique_years_months_dates = final_predicted_df[['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart']].drop_duplicates()

# Extract grid IDs from the grid mapping file
unique_grid_ids = grid_mapping_df['GridID'].unique()

# Generate all possible combinations of year, month, date part, and grid ID
all_combinations = product(unique_years_months_dates.itertuples(index=False), unique_grid_ids)
all_combinations_df = pd.DataFrame([(*ymd, grid_id) for ymd, grid_id in all_combinations],
                                   columns=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'])

# Merge the original data with all possible combinations using a left join
complete_data_df = pd.merge(all_combinations_df, final_predicted_df,
                            on=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'],
                            how='left')

# Fill NaN values in PredictedNumberOfIncidents with 0
complete_data_df['PredictedNumberOfIncidents'].fillna(0, inplace=True)

# Join with grid mapping data based on GridID
complete_data_with_grid_info_df = pd.merge(complete_data_df, grid_mapping_df, on='GridID', how='left')

# Sort the data for better organization
complete_data_with_grid_info_df.sort_values(by=['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart', 'GridID'], inplace=True)

# Reset the index
complete_data_with_grid_info_df.reset_index(drop=True, inplace=True)

# Now complete_data_with_grid_info_df contains the full dataset with grid mapping information

updated_dataframe_path = '/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted_grid.csv'
complete_data_with_grid_info_df.to_csv(updated_dataframe_path, index=False)

import pandas as pd
import folium
from shapely.wkt import loads
import imageio
import os

final_predicted_grid_df = pd.read_csv('/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted_grid.csv')

# Convert the geometry column from WKT to Shapely objects
final_predicted_grid_df['geometry'] = final_predicted_grid_df['geometry'].apply(loads)

# Get unique dates from the dataset
unique_dates = final_predicted_grid_df[['OccurrenceYear', 'OccurrenceMonth', 'OccurrenceDatePart']].drop_duplicates()

map_files = []

for _, unique_date in unique_dates.iterrows():
    # Filter data for the specific day
    day_data = final_predicted_grid_df[
        (final_predicted_grid_df['OccurrenceYear'] == unique_date['OccurrenceYear']) &
        (final_predicted_grid_df['OccurrenceMonth'] == unique_date['OccurrenceMonth']) &
        (final_predicted_grid_df['OccurrenceDatePart'] == unique_date['OccurrenceDatePart'])
    ]

    # Initialize map
    day_map = folium.Map(location=[33.4255, -111.9400], zoom_start=12)

    # Plot each grid's polygon and incidents on the map
    for _, row in day_data.iterrows():
        folium.GeoJson(
            row['geometry'],
            tooltip=f"Grid ID: {row['GridID']}<br>Incidents: {row['PredictedNumberOfIncidents']}"
        ).add_to(day_map)

    map_file = f'tempe_map_{unique_date["OccurrenceYear"]}_{unique_date["OccurrenceMonth"]:02d}_{unique_date["OccurrenceDatePart"]:02d}.html'
    day_map.save(map_file)
    map_files.append(map_file)

for filename in map_files:
    os.remove(filename)

for filename in map_files:
    os.remove(filename)

import plotly.express as px
import geopandas as gpd
import pandas as pd

file_path = '/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted_grid.csv'
data = pd.read_csv(file_path)

# Converting the 'geometry' column to a GeoDataFrame
data['geometry'] = gpd.GeoSeries.from_wkt(data['geometry'])
geo_data = gpd.GeoDataFrame(data, geometry='geometry')

# Function to create an interactive map for a selected date
def plot_interactive_map(selected_year, selected_month, selected_day):
    day_data = geo_data[(geo_data['OccurrenceYear'] == selected_year) &
                        (geo_data['OccurrenceMonth'] == selected_month) &
                        (geo_data['OccurrenceDatePart'] == selected_day)]

    fig = px.choropleth_mapbox(day_data, geojson=day_data.geometry.__geo_interface__,
                               locations=day_data.index, color='PredictedNumberOfIncidents',
                               color_continuous_scale="OrRd",
                               mapbox_style="carto-positron",
                               zoom=10, center={"lat": day_data.geometry.centroid.y.mean(),
                                                "lon": day_data.geometry.centroid.x.mean()},
                               opacity=0.5)

    fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
    fig.show()

plot_interactive_map(2023, 11, 9)

import plotly.express as px
import geopandas as gpd
import pandas as pd

file_path = '/content/drive/Shareddrives/dm_final/dm_final/data/test_gridcentroid_test_07x07_weekday.csv.xls'
data = pd.read_csv(file_path)

# Load the data from both CSV files
grid_mapping_path = '/content/drive/Shareddrives/dm_final/dm_final/data/grid_mapping_7x7.csv'
gridcentroid_path = '/content/drive/Shareddrives/dm_final/dm_final/data/test_gridcentroid_test_07x07_weekday.csv.xls'

grid_mapping = pd.read_csv(grid_mapping_path)
gridcentroid = pd.read_csv(gridcentroid_path)

# Understanding structure
grid_mapping_head = grid_mapping.head()
gridcentroid_head = gridcentroid.head()

grid_mapping_head, gridcentroid_head

# Merging the two DataFrames on the 'GridID' column
data = pd.merge(gridcentroid, grid_mapping, on='GridID', how='left')

data_head = data.head()
print(data_head)

# # Converting the 'geometry' column to a GeoDataFrame
data['geometry'] = gpd.GeoSeries.from_wkt(data['geometry'])
geo_data = gpd.GeoDataFrame(data, geometry='geometry')

# Function to create an interactive map for a selected date
def plot_interactive_map(selected_year, selected_month, selected_day):
    day_data = geo_data[(geo_data['OccurrenceYear'] == selected_year) &
                        (geo_data['OccurrenceMonth'] == selected_month) &
                        (geo_data['OccurrenceDatePart'] == selected_day)]

    fig = px.choropleth_mapbox(day_data, geojson=day_data.geometry.__geo_interface__,
                               locations=day_data.index, color='NumberOfIncidents',
                               color_continuous_scale="OrRd",
                               mapbox_style="carto-positron",
                               zoom=10, center={"lat": day_data.geometry.centroid.y.mean(),
                                                "lon": day_data.geometry.centroid.x.mean()},
                               opacity=0.5)

    fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
    fig.show()

plot_interactive_map(2023, 11, 9)

import plotly.express as px
import geopandas as gpd
import pandas as pd

file_path = '/content/drive/Shareddrives/dm_final/dm_final/data/final_predicted_grid.csv'
data = pd.read_csv(file_path)

# Converting the 'geometry' column to a GeoDataFrame
data['geometry'] = gpd.GeoSeries.from_wkt(data['geometry'])
geo_data = gpd.GeoDataFrame(data, geometry='geometry')

# Function to create an interactive map for a selected date
def plot_interactive_map(selected_year, selected_month, selected_day):
    day_data = geo_data[(geo_data['OccurrenceYear'] == selected_year) &
                        (geo_data['OccurrenceMonth'] == selected_month) &
                        (geo_data['OccurrenceDatePart'] == selected_day)]

    # Custom color scale: Green to Red
    # custom_color_scale = [("green", 0), ("yellow", 0.5), ("red", 1)]

    fig = px.choropleth_mapbox(day_data, geojson=day_data.geometry.__geo_interface__,
                               locations=day_data.index, color='PredictedNumberOfIncidents',
                               color_continuous_scale="icefire",
                               mapbox_style="carto-positron",
                               zoom=10, center={"lat": day_data.geometry.centroid.y.mean(),
                                                "lon": day_data.geometry.centroid.x.mean()},
                               opacity=0.5)

    fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
    fig.show()

plot_interactive_map(2023, 11, 11)